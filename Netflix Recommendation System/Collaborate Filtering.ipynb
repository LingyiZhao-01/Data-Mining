{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"recommendation-cf.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"ilBOU8PdaZyV"},"source":["################################### Summary ####################################\n","# Here we implement a onestop SVD (train a single SVD on the full dataset) and an ensembled SVD (train a collection of SVD partitioned by user_id).\n","# For each method, we implement methods for training, predicting, evaluation and hyperparameter tuning\n","\n","# TIME\n","# onestop - hyperparameter tuning (1M): ~01:00:00\n","# ensemble - weight tuning (100K): 17:00:00\n","# onestop - 5 fold CV (10M): ~00:20:00\n","# onestop - 5 fold CV (full dataset): ~03:00:00\n","# fit - onestop: ~01:20:00\n","# fit - ensemble: ~01:20:00\n","\n","# RESULT\n","# hyperparameters: n_factors = 50, n_epochs = 20, lr_all = 0.005, reg_all = 0.05\n","# onestop - 5 fold CV (1M) rmse: 0.9716\n","# onestop - 5 fold CV (10M) rmse: 0.9384\n","# onestop - 5 fold CV (full dataset): 0.8668\n","# ensembled - test rmse (1M data): 0.8770\n","# baseline: 1.5347"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZG2t1O7faZya"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","from surprise import Dataset, Reader\n","from surprise import SVD\n","from surprise.model_selection import train_test_split\n","from surprise.model_selection import cross_validate\n","from surprise.model_selection import RandomizedSearchCV\n","from sklearn.metrics import mean_squared_error\n","import seaborn as sns\n","\n","from datetime import datetime\n","import random\n","from joblib import dump, load\n","\n","data = pd.read_csv('.\\\\data\\\\data.csv', names=['movie_id','user_id','rating','date'])\n","netflix_id = pd.read_csv('.\\\\data\\\\netflix_id.csv',names=['id','year','title'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bN0KfeJFaZyb"},"source":["################################ Ensembled SVD #################################\n","class SVD_Ensemble:\n","    # these are the tuned parameters\n","    def fit(self, data, \n","            num_batch=10, \n","            n_factors=50,\n","            n_epochs=20,\n","            lr_all=0.005,\n","            reg_all=0.05):\n","        self.num_batch=num_batch\n","        self.movie_ids = data.movie_id.unique()\n","        self.user_ids = data.user_id.unique()\n","        self.user_ids.sort()\n","        \n","        num_user = len(self.user_ids)\n","        \n","        batch_size = int(num_user / num_batch)\n","        self.batch_user_index = {}\n","        for i in range(num_batch):\n","            if i == num_batch-1:\n","                self.batch_user_index[i] = self.user_ids[batch_size*(i):num_user]\n","            else:\n","                self.batch_user_index[i] = self.user_ids[batch_size*(i):batch_size*(i+1)]\n","        \n","        train = data[['user_id','movie_id','rating']]\n","        \n","        svds = {}\n","        reader = Reader()\n","        for i in range(num_batch):\n","            batch = self.batch_user_index[i]\n","\n","            is_batch = train['user_id'].isin(batch)\n","            train_batch = train[is_batch]\n","            \n","            svd = SVD(n_factors=n_factors,\n","                      n_epochs=n_epochs,\n","                      lr_all=lr_all,\n","                      reg_all=reg_all)\n","            data_sp = Dataset.load_from_df(train_batch, reader)\n","            train_batch_loaded = data_sp.build_full_trainset()\n","            \n","            svd.fit(train_batch_loaded)\n","            svds[i] = svd\n","        self.svds = svds  \n","    \n","    \n","    # if you just want to use the svd trained on the subset of users that contain the target user, then use weight=1\n","    # else take weighted average of all svds\n","    def predict_user(self, user, weight=0.8):\n","        \n","        assert(weight <= 1)\n","        \n","        # find the svd model trained on the user\n","        contains_user = [user in self.batch_user_index[i] for i in range(self.num_batch)]\n","        new_user = False\n","        if True not in contains_user:\n","            new_user = True\n","            weight = 1/self.num_batch\n","        else:\n","            user_batch_id = contains_user.index(True)\n","            \n","        # if a new user, ensemble svds by average\n","        if new_user:\n","            ratings = []\n","            for i in range(self.num_batch):\n","                rating = [self.svds[i].predict(user, mid).est for mid in self.movie_ids]\n","                ratings.append(rating)\n","            rating_avg = np.average(ratings, axis=0)\n","        \n","        # if weight=1, then use a single svd trained on the user_id\n","        elif weight == 1:\n","            rating_avg = [self.svds[user_batch_id].predict(user, mid).est for mid in self.movie_ids]\n","        else:\n","            # if weight < 1, then use an ensemble of svd\n","            user_weight = weight\n","            else_weight = (1-weight)/(self.num_batch-1)\n","            \n","            ratings = []\n","            for i in range(self.num_batch):\n","                rating = [self.svds[i].predict(user, mid).est for mid in self.movie_ids]\n","                if i == user_batch_id:\n","                    rating = np.multiply(rating, user_weight)\n","                else:\n","                    rating = np.multiply(rating, else_weight)\n","                ratings.append(rating)\n","            rating_avg = list(np.sum(ratings, axis=0))\n","\n","        output = pd.DataFrame({'rating_pred':rating_avg, \n","                               'movie_id':list(self.movie_ids), \n","                               'user_id':[user]*len(self.movie_ids)})\n","        return output\n","    \n","    # a wrapper for predict_user() to predict for a list of users\n","    def predict(self, users, weight=0.8):\n","        preds = []\n","        for user in users:\n","            pred = self.predict_user(user, weight=weight)\n","            preds.append(pred)\n","        return pd.concat(preds,axis=0)\n","\n","    # model evaluation       \n","    def evaluate(self, data, test_size = 0.2, weight=0.8):\n","        testset = data.sample(frac=test_size)\n","        trainset = data.drop(testset.index)\n","        print('retrieve svds')\n","        try:\n","            self.svds\n","        except:\n","            print(1)\n","            self.fit(trainset)\n","        y_pred = self.predict(testset.user_id.unique(),weight=weight)\n","        eval_df = pd.merge(y_pred, testset, how='inner', on=['user_id','movie_id'])\n","        mse = np.mean(np.power((eval_df['rating'] - eval_df['rating_pred']),2))\n","        rmse = np.sqrt(mse)\n","        return rmse"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fJ83pOZaZyb"},"source":["############################ Exploratory Data Analysis #########################\n","def exploratory(data):\n","    # construct pivot table using only 10M data to see how much data is actually missing\n","    data_pivot = data[1:10000001].pivot(index='user_id',columns='movie_id',values='rating')\n","    ct_na = data_pivot.isna().sum().sum()\n","    pct_na = ct_na/(ct_na+10000000)\n","    print(f'% rating filled: {(1-pct_na)*100: .2f}%')\n","    \n","    col_ct = 'Rating Count'\n","    review_ct_by_movie = data.groupby('movie_id').size().reset_index(name=col_ct)\n","    review_ct_by_user = data.groupby('user_id').size().reset_index(name=col_ct)\n","    \n","    sns.kdeplot(data=review_ct_by_movie,x=col_ct).set_title('Rating Density by Movie')\n","    sns.kdeplot(data=review_ct_by_user,x=col_ct).set_title('Rating Density by User')\n","    \n","    print('Rating count by movie - mean:', review_ct_by_movie[col_ct].mean())\n","    print('Rating count by movie - sd:', review_ct_by_movie[col_ct].std())\n","    print('Rating count by user - mean:', review_ct_by_user[col_ct].mean())\n","    print('Rating count by user - sd:', review_ct_by_user[col_ct].std())\n","    \n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERQqz2mQaZyc"},"source":["########################### Tuning SVD hyperparameters #########################\n","## RESULT: n_factors = 50, n_epochs = 20, lr_all = 0.005, reg_all = 0.05\n","def hyperparameter_tuning(data):\n","    reader = Reader()\n","    data_rs = Dataset.load_from_df(data, reader)\n","    \n","    parameters = {'n_factors':[50,100,200],\n","                  'n_epochs':[20,40],\n","                  'lr_all':[0.005,0.001],\n","                  'reg_all':[0.05,0.02,0.01]}\n","    rs = RandomizedSearchCV(SVD, parameters, measures=['rmse'], cv=5)\n","    rs.fit(data_rs)\n","    \n","    return rs.best_params['rmse'], rs.best_score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF28koWWaZyc"},"source":["########################### Tuning Ensembled SVD Weights #########################\n","## RESULT: weight = 1\n","def weight_tuning_ensemble(data):\n","    weights = [0.3,0.5,0.7,0.9,1]\n","    rmses = {}\n","    trainer = SVD_Ensemble()\n","    for w in weights:\n","        print(w)\n","        rmses[w] = trainer.evaluate(data[0:1000000],weight=w)\n","    return rmses"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sjbj9xt9aZyd"},"source":["################################# Onestop SVD ##################################\n","def train(data):\n","    reader = Reader()\n","    svd = SVD()\n","\n","    data_sp = Dataset.load_from_df(data[['user_id','movie_id','rating']], reader)\n","    train = data_sp.build_full_trainset()\n","    svd.fit(train)\n","    return svd\n"," \n","def predict(svd, user):\n","    rating_pred = [svd.predict(user, mid).est for mid in netflix_id.id]\n","    pred_user = pd.concat([netflix_id,pd.Series(rating_pred,name='rating_pred')], axis=1).sort_values('rating_pred', ascending=False)\n","    return pred_user"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IDWGxZ33aZyd"},"source":["########################### Prediction Visualization ###########################\n","def pred_visualization():\n","    model = load('..//trained//cf-svd.joblib')\n","    # randomly pick 230 users, they could be in the training set or new users\n","    pred = [{'user_id':i,'recommend': predict(model,i)} for i in range(1,100000000, 431823)]\n","    return pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CEBr7ZdmaZyd"},"source":["############################### Model Evaluation ###############################\n","## SEE RESULT AT TOP\n","def model_evaluation(data, method):\n","    \n","    assert((method == 'onestop') | (method == 'ensemble'))\n","    \n","    if method == 'onestop':\n","        svd = SVD(n_factors=50,\n","              n_epochs=20,\n","              lr_all=0.005,\n","              reg_all=0.05)\n","        \n","        reader = Reader()\n","        data_sp_cv = Dataset.load_from_df(data[['user_id','movie_id','rating']], reader)\n","        svd_cv = cross_validate(svd, data_sp_cv, measures=['rmse'],cv=5,verbose=True)\n","    else:\n","        trainer = SVD_Ensemble()\n","        trainer.evaluate(data[0:1000000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Npihn2BUaZye"},"source":["########################### Baseline Model Evaluation ###########################\n","## RESULT: rmse = 1.5347\n","def baseline_model_rmse(data):\n","    baseline = data['rating'].value_counts(normalize=True).sort_index()\n","    testset = data.sample(frac=0.2)\n","    pred = random.choices([1,2,3,4,5],baseline,k=len(testset))\n","    mse = np.mean(np.power((testset['rating'] - pred),2))\n","    rmse = np.sqrt(mse)\n","    print(rmse)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KaMlDcDEaZye"},"source":["def main(method):\n","    \n","    assert((method == 'onestop') | (method == 'ensemble'))\n","    \n","    if method == 'onestop':\n","        svd = train(data)\n","        dump(svd, 'cf-svd.joblib')\n","    elif method == 'ensemble':\n","        trainer = SVD_Ensemble()\n","        trainer.fit(data)\n","        for i in range(len(trainer.svds)):\n","            dump(trainer.svds[i], f'..//trained-ensemble//cf-svd-ensemble-{i}.joblib')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8pedDgLFaZye"},"source":["if __name__ == \"__main__\":\n","    \n","    exploratory(data)\n","    params = hyperparameter_tuning(data[['user_id','movie_id','rating']][0:1000000])\n","\n","    baseline_model_rmse(data)\n","\n","    main('onestop')\n","    \n","    pred = pred_visualization()\n","\n","    model_evaluation(data[0:1000000],'onestop')"],"execution_count":null,"outputs":[]}]}